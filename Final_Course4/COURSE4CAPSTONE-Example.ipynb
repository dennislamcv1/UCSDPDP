{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Example\n",
    "\n",
    "Last week's notebook discussed a cummulative project that would be used as a test of knowledge from this series of courses.\n",
    "This notebook will serve as a reference point for you while you work on said project. Included in this notebook is a set of answers to your tasks, based on a set dataset. Make sure in your final submission you are using a different dataset! \n",
    "\n",
    "You will be working on 4 tasks:\n",
    "1. __Data Processing__ \n",
    "2. __Classification__ \n",
    "3. __Regression__ \n",
    "4. __Recommender Sytstems__\n",
    "\n",
    "These tasks are each representative of one of the courses in the series. So if you need help with any one of these tasks, be sure to look back at those courses for reference! Along with the previous courses, there will be checkpoints with given solutions so you can check to make sure you are headed in the right direction. ___Good Luck!___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing\n",
    "\n",
    "## The Data\n",
    "\n",
    "For this final project you will be doing your work on a dataset of your choice. For reference, an example with checkpoint answers will be included. This example will be an amazon dataset, which does not need any cleaning before proper analysis. This dataset in particular can be found [here.](https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz)\n",
    "This dataset is a set of Home Improvement Product reviews on amazon. It is a rather large dataset, so our computation might take slightly longer than normal.\n",
    "\n",
    "### First Step: Imports\n",
    "\n",
    "In the next cell we will give you all of the imports you should need to do your project. Feel free to add more if you would like, but these should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy\n",
    "import scipy.optimize\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from nltk.stem.porter import PorterStemmer # Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Read the data and Fill your dataset\n",
    "\n",
    "Take care of int casting the votes and rating. Also __add this bit of code__ to your for loop, taking off the outer \" \":\n",
    "\n",
    "\" d['verified_purchase'] = d['verified_purchase'] == 'Y' \"\n",
    "\n",
    "This simple makes the verified purchase column be strictly true/false values rather than Y/N strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this setup properly, you __should__ shuffle your data (which you should do in your submission), but the checkpoint values would change so for the sake of this example we will ___not___ shuffle the data.\n",
    "\n",
    "### TODO 2: Split the data into a Training and Testing set\n",
    "\n",
    "Have Training be the first 80%, and testing be the remaining 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2107824 526957\n",
      "Lengths should be: 2107824 526957\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "print(len(trainingSet), len(testSet))\n",
    "print(\"Lengths should be: 2107824 526957\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now delete your dataset\n",
    "You don't want any of your answers to come from your original dataset any longer, but rather your Training Set, this will help you to not make any mistakes later on, especialy when referencing the checkpoint solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Extracting Basic Statistics\n",
    "\n",
    "Next you need to answer some questions through any means (i.e. write a function or just find the answer) all based on the __Training Set:__\n",
    "1. What is the __average rating__?\n",
    "2. What fraction of reviews are from __verified purchases__?\n",
    "3. How many __total users__ are there?\n",
    "4. How many __total items__ are there?\n",
    "5. What fraction of reviews have __5-star ratings__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "1. 4.219492709068689\n",
    "2. 0.9176558384381238\n",
    "3. 1396587\n",
    "4. 294787\n",
    "5. 0.642813631498645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Classification\n",
    "\n",
    "Next you will use our knowledge of classification to extract features and make predictions based on them. Here you will be using a Logistic Regression Model, keep this in mind so you know where to get help from.\n",
    "\n",
    "### TODO 1: Define the feature function\n",
    "\n",
    "This implementation will be based on the __star rating__ and the ___length___ of the __review body__. Hint: Remember the offset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Fit your model\n",
    "\n",
    "1. Create your __Feature Vector__ based on your feature function defined above. \n",
    "2. Create your __Label Vector__ based on the \"verified purchase\" column of your training set.\n",
    "3. Define your model as a __Logistic Regression__ model.\n",
    "4. Fit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Compute Accuracy of Your Model\n",
    "\n",
    "1. Make __Predictions__ based on your model.\n",
    "2. Compute the __Accuracy__ of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Finding the Balanced Error Rate\n",
    "\n",
    "1. Compute __True__ and __False Positives__\n",
    "2. Compute __True__ and __False Negatives__\n",
    "3. Compute __Balanced Error Rate__ based on your above defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "3. Accuracy = 0.9172573231920692\n",
    "4. BER = 0.4895446422238072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Regression\n",
    "\n",
    "In this section you will start by working though two examples of altering features to further differentiate. Then you will work through how to evaluate a Regularaized model.\n",
    "\n",
    "Lets start by defining a new y vector, specific to our Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = [d['star_rating'] for d in trainingSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Unique Words in a Sample Set\n",
    "\n",
    "We are going to work with a smaller Sample Set here, as stemming on the normal training set will take a very long time. (Feel free to change sampleSet -> trainingSet if you would like to see)\n",
    "\n",
    "1. Count the number of unique words found within the 'review body' portion of the sample set defined below, making sure to __Ignore Punctuation and Capitalization__.\n",
    "2. Count the number of unique words found within the 'review body' portion of the sample set defined below, this time with use of __Stemming,__ __Ignoring Puctuation,__ ___and___ __Capitalization__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#GIVEN for 2.\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSet = trainingSet[:2*len(trainingSet)//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Evaluating Classifiers\n",
    "\n",
    "1. Given the feature function and your counts vector, __Define__ your X_reg vector. (This being the X vector, simply labeled for the Regression model)\n",
    "2. __Fit__ your model using a __Ridge Model__ with (alpha = 1.0, fit_intercept = True).\n",
    "3. Using your model, __Make your Predictions__.\n",
    "4. Find the __MSE__ between your predictions and your y_reg vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN FUNCTIONS\n",
    "def feature_reg(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    return feat\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN COUNTS AND SETS\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#Note: increasing the size of the dictionary may require a lot of memory\n",
    "words = [x[1] for x in counts[:100]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "1. len(wordCount) = 135769\n",
    "2. len(wordCountStem) = 113888\n",
    "4. MSE = 1.2869981011943792 (Roughly, could change slightly due to rounding errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to work with this example more in your free time, here are some tips to improve your solution:\n",
    "# 1. Implement a validation pipeline and tune the regularization parameter\n",
    "# 2. Alter the word features (e.g. dictionary size, punctuation, capitalization, stemming, etc.)\n",
    "# 3. Incorporate features other than word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Recommendation Systems\n",
    "\n",
    "For your final task, you will use your knowledge of simple latent factor-based recommender systems to make predictions. Then evaluating the performance of your predictions.\n",
    "\n",
    "### Starting up\n",
    "\n",
    "The next cell contains some starter code that you will need for your tasks in this section.\n",
    "Notice you are back to using the __trainingSet__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fill our default dictionaries for our dataset\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for d in trainingSet:\n",
    "    user,item = d['customer_id'], d['product_id']\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    \n",
    "#Create two dictionaries that will be filled with our rating prediction values\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "#Getting the respective lengths of our dataset and dictionaries\n",
    "N = len(trainingSet)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nItems = len(reviewsPerItem)\n",
    "\n",
    "#Getting the list of keys\n",
    "users = list(reviewsPerUser.keys())\n",
    "items = list(reviewsPerItem.keys())\n",
    "\n",
    "### You will need to use this list\n",
    "y_rec = [d['star_rating'] for d in trainingSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Calculate the ratingMean\n",
    "\n",
    "1. Find the __average rating__ of your training set.\n",
    "2. Calculate a __baseline MSE value__ from the actual ratings to the average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are defining the functions you will need to optimize your MSE value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN\n",
    "alpha = ratingMean\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d['customer_id'], d['product_id']) for d in trainingSet]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(trainingSet)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in trainingSet:\n",
    "        u,i = d['customer_id'], d['product_id']\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - d['star_rating']\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Optimize\n",
    "\n",
    "1. __Optimize__ your MSE using the scipy.optimize.fmin_1_bfgs_b(\"arguments\") functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "1. ratingMean = 4.219492709068689\n",
    "2. baseLine = 1.634495697493549\n",
    "3. optimized MSE -> converges to roughly 1.6083083....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're all done!\n",
    "\n",
    "Congratulations! This project was the end of 4 whole courses worth of content! This project clearly didn't cover every single topic from those courses, but it serves as a summary for everything you have learned. This is only the start of Python Data Projects, so continue to learn and good luck in your future endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
